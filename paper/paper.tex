%-----------------------------------------------------------------------------
%
%               Template for sigplanconf LaTeX Class
%
% Name:         sigplanconf-template.tex
%
% Purpose:      A template for sigplanconf.cls, which is a LaTeX 2e class
%               file for SIGPLAN conference proceedings.
%
% Guide:        Refer to "Author's Guide to the ACM SIGPLAN Class,"
%               sigplanconf-guide.pdf
%
% Author:       Paul C. Anagnostopoulos
%               Windfall Software
%               978 371-2316
%               paul@windfall.com
%
% Created:      15 February 2005
%
%-----------------------------------------------------------------------------


\documentclass{sigplanconf}

% The following \documentclass options may be useful:
%
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.

\usepackage{amsmath}
\usepackage[pdftex]{graphicx}
\usepackage{listings}

\begin{document}

\conferenceinfo{WXYZ '05}{date, City.} 
\copyrightyear{2011} 
\copyrightdata{[to be supplied]} 

\titlebanner{banner above paper title}        % These are ignored unless
\preprintfooter{short description of paper}   % 'preprint' option specified.

\title{Python to Assembly Compilation}
\subtitle{With Reference Counting Garbage Collection}

\authorinfo{Brent Smith}
           {University of Colorado, Boulder}
           {brent.m.smith@colorado.edu}
\authorinfo{Robert Elsner}
           {University of Colorado, Boulder}
           {robert.elsner@colorado.edu}

\maketitle

\begin{abstract}
Garbage collection in high level languages has enabled increased programmer productivity, but at the cost of performance and ease of implementation for the language designer.  This paper will explore these trade-offs through an implementation of automatic garbage collection for a subset of the Python language.   Our implementation will extend an existing Python to x86 compiler to use reference counting garbage collection.  Reference counting is chosen for its ease of implementation, and because it is less invasive when one of the design goals is to integrate with assembly or C routines without extra intervention from the software developer.  We outline the changes to the existing compiler to enable reference counting, including how the abstract syntax tree of the program is altered, and then discuss the trade-offs made by our design choices.
\end{abstract}

\category{D.1.5}{Programming Techniques}{Object-oriented Programming}
\category{D.3.3}{Programming Languages}{Language Constructs And Features-Dynamic Storage Management}
\category{D.3.4}{Programming Languages}{Processors- Memory Management (garbage collection)}
\category{D.4.2}{Operating Systems}{Storage Management-Garbage collection}

\terms
Management, Performance, Design, Languages, Algorithms

\keywords
memory management, garbage collection, reference counting, compilation, compilers, programming languages

\section{Introduction}

Python is a modern, high level language that uses dynamic typing and is run within an interpreter.  It manages memory for the programmer, and is considered by many to be a good ``prototyping'' language, in which programmers can quickly and easily develop an application.   Automatic memory management plays a large part in supporting this role, as it relieves the programmer from the tedious task of remembering when to de-allocate memory.  For language designers, deciding what form of memory management to incorporate into their language can be a difficult task.  The choice will depend on many factors, including the designer's vision on the general usefulness of the language, the environment where the language will be primary used, performance requirements, program semantics, integration with legacy systems and many other factors.  The choice also has significant implications on the implementation and therefore warrants careful consideration.  

There are currently two widely used strategies to perform automatic memory management.  One such strategy is tracing garbage collection, which periodically traverses the in-memory object graph starting from a currently reachable set of ``root'' objects.  This set of objects includes all global variables, as well as the objects which are referenceable from the current stack frame.  Any objects that are traversed during this operation are considered reachable and therefore cannot be de-allocated.  Conversely, any objects not traversed are no longer reachable, and can be de-allocated.  The other primary strategy for automated memory management, is reference counting garbage collection. Reference counting works by keeping a count of references to a particular object.  References are tracked by incrementing and decrementing an object's reference count when an assignment to a pointer occurs, or when a pointer variable goes out of scope.  When the reference count reaches zero, it is safe to de-allocate the object.

The performance characteristics of each strategy can vary considerably, depending on implementation.   Tracing garbage collectors can sometimes be associated with long pause times, where application work must be temporarily stopped to allow the entire contents of the heap to be scanned by the garbage collector.  These long pause times have largely been solved with new algorithms that improve upon the original naive mark-and-sweep algorithm.  It is typical that these algorithms tradeoff throughput for responsiveness.  However, these pause times can still be an issue, especially in real time systems, where a hard guarantees on latency are needed.  Reference counting has been discussed as a solution to this problem, but classic implementations of reference counting can have serious performance concerns when one de-allocation results in cascading de-allocations\cite{boehm}.  

Given an existing implementation of a Python to x86-assembly compiler which lacks any memory management, we explore the consequences of adding a reference counting garbage collector.  Our base Python-like language includes classes, objects, functions, lambdas, addition, unary subtraction, lists, dictionaries and integer primitives.  The remainder of the paper is organized as follows.  Section~\ref{sec:related} discusses related work.  Section~\ref{sec:implementation} discusses the implementation of reference counting for our compiler.  Section~\ref{sec:results} evaluates our compiler for performance and correctness.  Section~\ref{sec:future} discusses potential future work.  Finally, section~\ref{sec:conclusion} concludes.


\section{Related Work}
\label{sec:related}
In general a reference count garbage collector will have shorter pause times for the garbage collection phase, but will incur a higher performance penalty \cite{joisha}\cite{blackburn}.  Reference-counting collectors have been implemented which defer the de-allocation, called lazy reference-counting, to some later time and some improvements on such strategies have been made \cite{boehm}.  Without extra work a strictly reference-counting collector will leak memory when references contain cycles, and multiple algorithms have been presented as solutions to this problem. \textit{Citation?}

One potential advantage that reference-counting garbage collectors have is the ability to be implemented in hardware \cite{joao}.  Given the significant cost of garbage collection \cite{hertz} any hardware assisted acceleration would be highly desirable if it is flexible enough to adapt across multiple garbage collection techniques. 

Real-time systems are rarely implemented in languages which are garbage collected, but reference-counting garbage collection has been adapted to hard real-time systems \cite{ritzaou}.  Due to immediate knowledge that an object is no longer in use reference counting garbage collection can allow for destructors or finalizers to be run immediately which can enhance the clean up of system resources.

In \cite{levanoni}, Levanoni, et. al. show that reference counting is a valid approach for multprocessor systems.

\par
\section{Implementation}
\label{sec:implementation}

We start with an existing compiler based upon previous work\cite{siek}.  This compiler converts a subset of the Python language to x86 assembly and supports one of the core features of Python, namely that everything is a first class object.  Prior to our modifications, the compiler does not perform any memory management, resulting in a language that is only useful for short lived programs, with minimal memory constraints.  After our modifications, the compiler properly maintains reference counts for all objects created, and automatically de-allocates objects when there are no longer any references to them.

Implementing reference counting requires a minimum set of steps to be taken.

\begin{itemize}
\item Objects must include a counter that tracks the number of references to that object.
\item Functions to increment and decrement an objects reference count must be created.
\item Assignment of an object to a pointer or \textit{reference} should result in correct updates to an object's reference count.
\item Reference counts must be decremented when a program reaches the end of a given scope, since local variables are destroyed.
\item When an object's reference count reaches zero, it is no longer reachable and can be safely destroyed.

\end{itemize}

These steps require modifications to the architecture of our compiler, as well as the supporting runtime library responsible for creating objects and allocating memory.  We discuss these modifications in the following sections.

\subsection{Runtime Modifications}
Our runtime library is implemented in C so that it can be easily called by the assembly programs created by our compiler.  The runtime library contains abstractions such as \texttt{create\_list}, \texttt{create\_dict} and \texttt{create\_object} that take care of allocating and initializing memory for a new object of the specified type.  These functions return a pointer to the memory allocated, and the pointer is \textit{tagged} to indicate that it represents a non-primitive type.  In our reference compiler, any non-primitive object must be a pointer to a C structure called \texttt{big\_pyobj}.  To support reference counts, this C structure is modified to include an int field called \texttt{ref\_ctr}.  Then all of the functions to create objects are modified to set the initial reference count to zero.

To support incrementing and decrementing an object's reference count, we create corresponding functions in our runtime called \texttt{inc\_ref\_ctr} and \texttt{dec\_ref\_ctr}.  The increment function is trivial in that it simply de-references the object pointer and increments the \texttt{ref\_ctr} field.   However, the decrement function potentially has more work to do than just de-referencing the pointer and decrementing the reference count.  According to the steps for reference counting mentioned previously, we need to de-allocate an object whenever its reference count reaches zero.  To provide this behavior, the \texttt{dec\_ref\_ctr} function has to check if the reference count has reached zero, and if so, delegate to a corresponding \texttt{free\_\textit{type}} method to free an object of the given type.

In the \texttt{free\_\textit{type}} functions, it is necessary to add further calls to decrement the reference counter of any objects referenced by the object being de-allocated.  For the case of a list, we have the \texttt{free\_list} function, which iterates over each element in the list and decrements its reference count.  For a dictionary, we have the \texttt{free\_dict} function, which decrements the reference counter on keys as well as values contained in the dictionary, since both can be objects.  Note that for any case of an object that references other objects, this can can result in de-allocation of the referenced object, if the only reference to it is held by the parent object.  In the worst case, this could lead to a number of de-allocations equivalent to the number of nodes in the object graph created by the parent object.  

Since every object in our system may contain references to another object, we modify our runtime to correctly increment the reference count when such as reference is established.  For example, an assignment to a list or dictionary subscript results in a call to the \texttt{set\_subscript} runtime function.  This function is modified to first decrement the reference count for any object that may have been previously referenced by the given subscript, and then increment the reference count for the new object that is assigned to the given subscript.  Similarly, a closure maintains a reference to its list of free variables, and a class object maintains a reference to its base classes.  In each case, we modify the appropriate runtime function to ensure that these reference counts are correctly maintained.  

In some cases, the type definition for an object in our runtime did not aggregate a \texttt{big\_pyobj} type, and instead maintained only a pointer to the non-generic version of the type (e.g., \texttt{class\_struct}).  For this reason, significant changes were made to the runtime to ensure that any type that references another type, has a generic pointer to a \texttt{big\_pyobj} structure, so that the reference counts can be correctly maintained.  To ensure that we did not make any mistakes in this process, we create a runtime test suite that uses the abstractions provided by the runtime library to create objects, increment and decrement their reference counts and ensure that all objects are de-allocated when their reference count reaches zero.  This runtime test suite is important to unit test the runtime library, before attempting to use it from our compiler.

Finally, in order to verify the correctness of our implementation, we create a set of abstractions on top of the system allocation functions \texttt{malloc} and \texttt{free}.  These abstractions are part of our runtime and provide us the following benefits. 

\begin{enumerate}
\item Ability to detect memory leaks
\item Ability to track allocations by type/size
\item Ability to track the time of allocation/de-allocation
\end{enumerate}

The abstractions are implemented in a new module appropriately called \texttt{pymem}.  Every allocation of an object in our runtime is modified to use the \texttt{pymem\_new} and \texttt{pymem\_free} instead of \texttt{malloc} and \texttt{free}. As a result of this, we can determine exactly how much memory has been allocated and what objects have still not been de-allocated, by calling the \texttt{pymem\_print\_statistics} function at any given program point.  In practice, this is only called at the end of each program to ensure that all memory has been de-allocated.  Since we can track the time of allocation and de-allocation, this module can be beneficial for investigating object lifetime as well.  

The \texttt{pymem\_new} routine works by allocating slightly more memory than is requested by the user.  This extra memory is used to store a pointer to a linked list node that contains the details of the allocation, including the type of object being created, the requested size, the pointer returned by \texttt{malloc} and the time of allocation.  The caller is then returned the portion of memory immediately after this pointer.  Subsequent calls to \texttt{pymem\_free} then have access to the linked list node via the pointer, and can update the node to mark the memory as de-allocated, as well as the de-allocation time.  The memory can then be freed independently of the linked list containing the statistics. 

\subsection{Compiler}

Figure~\ref{fig-comparch} shows the compiler data flow after modifying our compiler for reference counting.  Each stage operates on an abstract syntax tree (AST).  The relevant new stages are the pre-explicate flatten stage, and the reference counting AST transformation stage.  

\begin{figure}
\begin{center}
\includegraphics[scale=0.48]{compiler_flow.pdf}
\end{center}
\caption{Compiler Architecture}
\label{fig-comparch}
\end{figure}

\subsubsection{Reference Counting AST Transformation}

Recall that in order to properly maintain reference counts, we need to increment the reference count for an object when a reference is created for it, and decrement the reference count when a reference is destroyed.  References are created by assigning a memory location (which contains an object) to a variable.  References are destroyed when a variable is changed to refer to a different object, or when a variable ceases to exist when a program reaches the end of a given scope.

The most natural way to implement this in our compiler, is to transform our input programs AST such that any assignment to a variable is preceded by a decrement to the reference count for the object the variable refers to, and is succeeded by an increment to the reference count for the newly assigned object.  This is the approach we took, but there are some subtleties with respect to where this operation is placed within the compiler.  Our initial hypothesis was that we could easily add the increment and decrement reference counter operations in the existing ``Flatten AST'' stage.  We quickly realized that because this stage is after type explication, there exist variables which are not \textit{tagged} with a type.  Since we only want to decrement reference counts for \textit{tagged} types\footnote{Actually, we only want to modify reference counts for a subset of tagged types that only includes ``big'' types. However, to simplify our implementation, we transform our AST to always call our increment and decrement runtime functions, but these functions will ignore the primitive types of \texttt{int} and \texttt{boolean}.}, the only option is to put this transformation stage before type explication.  In addition to modifying the AST prior before/after an assignment, the reference counting transformation is also responsible for determining which variables are local to a given scope and decrementing the counts for these references at the end of the given program scope.

\subsection{Challenges}
We encountered a number of interesting challenges related to our design choices in modifying the compiler and summarize them below.

\subsubsection{Expressions}
Prior to our explicate phase, complex expressions are not flattened.  This leads to problems where memory is leaked, since no assignments ever occur to temporary objects.  Listing~\ref{lst:simpleleak} shows a program that assigns the result of addition between to lists to the variable \texttt{a}.  The assignment to the variable \texttt{a} results in a corresponding increment reference count on the result of the list addition.  However, the temporary lists, \texttt{[1]} and \texttt{[2]} are leaked if we don't take any additional steps.  To overcome this problem, we introduce a intermediate flattening stage, prior to our reference counting AST transformation.   This results in the corresponding code given in Listing~\ref{lst:flattened}.

\begin{figure}[h!]
\begin{lstlisting}[
language=Python,
frame=topline,
caption={A simple input program that leaks memory if not flattened.},
captionpos=b,
label=lst:simpleleak
]
a = [1] + [2]
a = 0
print a
\end{lstlisting}
\end{figure}

\begin{figure}[h!]
\begin{lstlisting}[
language=Python,
frame=topline,
caption={The flattened version of listing \ref{lst:simpleleak}.},
captionpos=b,
label=lst:flattened
]
tmp0 = [1]
tmp1 = [2]
tmp2 = tmp0 + tmp1
a = tmp2
a = 0
print a
\end{lstlisting}
\end{figure}

\subsubsection{Temporary variables}
As a result of introducing the intermediate flattening stage, we introduce additional references to an object which hang around until the end of the given program scope.  This is undesirable because it changes the de-allocation behavior of the program from the expected behavior.  For original input program in figure~\ref{lst:simpleleak}, the expectation is that when the variable \texttt{a} is assigned the value zero, the reference count for the list object should be decremented to zero and the object de-allocated.  However, in the flattened program, this is no longer the case since the \texttt{tmp2} variable continues to reference the object.  The de-allocation is therefore delayed until the end of the current scope.  To handle this situation, we keep track of all temporary variables introduced as a result of flattening an expression.  These temporary variables are then assigned a value of zero immediately after the statement containing the expression in which they were introduced.  This will lead to an immediate decrement of the reference count after the reference counting AST transformation.  Figure~\ref{lst:fixflattened} shows the result of the flattening transformation after this change is made.

\begin{figure}[h!]
\begin{lstlisting}[
language=Python,
frame=topline,
caption={The flattened version of listing \ref{lst:simpleleak} with immediate de-allocation of temporary variables.},
captionpos=b,
label=lst:fixflattened
]
tmp0 = [1]
tmp1 = [2]
tmp2 = tmp0 + tmp1
a = tmp2
tmp2 = 0
tmp1 = 0
tmp0 = 0
a = 0
print a
\end{lstlisting}
\end{figure}


\subsubsection{Variable Assignment}
We wanted a simple way to determine when to decrement an objects reference counter.  Our first implementation was to add a call to decrement the counter before every assignment.  However, because a variable can be created at any point in program execution we couldn't just blindly decrement the counter of an object since the variable will be uninitialized, and could be pointing at memory that makes it look like an object or accidentally references an object.
\par
The solution we took makes a slight modification to program behaviour, but we feel it is an acceptable trade-off.  In every scope, the compiler adds an immediate assignment as the first instructions.  This assignment is to a primitive, so the cost is minimal and then a decrement on the reference counter amounts to a function call with a no-op.  This function call overhead could potentially be avoided by adding code to check if the variable points to an object or a primitive and is a future enhancement for performance.  A program which used uninitialized (or declared) variables for program flow control would then have its runtime semantics changed.  We felt this was a sufficient trade off since such use of an exception for program flow control is frowned upon in practice and the benefit to our design was an obvious positive.

\subsubsection{Addition of Lists}
In the current runtime an entirely new list is created when two lists are added together.  At the assignment level our compiler can determine (due to the flattened structure) that the left-hand-side and the right-hand-side of the addition need their reference counters manipulated, however the assignment from an addition to a list creates an entirely new list which should properly increment the reference counter to all objects in both lists.

\subsubsection{Functions}

The compiler creates temporary variables in a number of situations (namely function calls and as a result of flattened expressions).  These temporary variables need not hang on to their memory reference past the next assignment to the actual result variable.  Any temporary variable is dereferenced immediately at the end of the current scope if it is not the variable being returned.  If it is the return variable we ignore decrementing the reference on this variable, thereby assuring the caller that the object returned is correctly reference counted for its assignment.  If we had decremented the reference counter, due to the immediate nature of garbage collection, we would have been returning a value which did not point to an allocated memory region.

\subsection{Assembly Output}
We modified the x86 assembly to include calls to set up our memory tracking system and print statistics at the end of the program.  Running analysis tools such as valgrind on our programs before and after shows the obvious gain of going from every memory allocation leaks to zero leaks.

\section{Results}
\label{sec:results}

\subsection{Execution}

\subsection{Memory}

\subsubsection{Valgrind results}
For the trivial program
\begin{verbatim}
i = 0
list = []

while i !=  10000:
    list = list + [[i]]
    i = i + 1

\end{verbatim}
Without any garbage collection, valgrind reports 
\begin{verbatim}
==23467== LEAK SUMMARY:
==23467==    definitely lost: 560,028 bytes in 20,001 blocks
==23467==    indirectly lost: 199,004,992 bytes in 20,191 blocks
==23467==      possibly lost: 1,375,008 bytes in 19,810 blocks
\end{verbatim}
With our reference counting for the same program
\begin{verbatim}
==23831== LEAK SUMMARY:
==23831==    definitely lost: 0 bytes in 0 blocks
==23831==    indirectly lost: 0 bytes in 0 blocks
==23831==      possibly lost: 0 bytes in 0 blocks
\end{verbatim}

\section{Future work}
\label{sec:future}

Optimizations.

\section{Conclusions}
\label{sec:conclusion}

\subsection{Disadvantages}

\subsection{Advantages}

%\appendix
%\section{Appendix Title}
% This is the text of the appendix, if you need one.

\acks

We would like to thank Jeremy Siek and Evan Chang for the course notes from which the compiler design is derived.

% We recommend abbrvnat bibliography style.

\bibliographystyle{abbrvnat}

% The bibliography should be embedded for final submission.

\begin{thebibliography}{}
\softraggedright
\bibitem{joisha}
Pramod G. Joisha. 2006. Compiler optimizations for nondeferred reference: counting garbage collection. In Proceedings of the 5th international symposium on Memory management (ISMM '06). ACM, New York, NY, USA, 150-161. DOI=10.1145/1133956.1133976

\bibitem{blackburn}
Stephen M. Blackburn and Kathryn S. McKinley. 2003. Ulterior reference counting: fast garbage collection without a long wait. In Proceedings of the 18th annual ACM SIGPLAN conference on Object-oriented programing, systems, languages, and applications (OOPSLA '03). ACM, New York, NY, USA, 344-358. DOI=10.1145/949305.949336

\bibitem{boehm}
Hans-J. Boehm. 2004. The space cost of lazy reference counting. In Proceedings of the 31st ACM SIGPLAN-SIGACT symposium on Principles of programming languages (POPL '04). ACM, New York, NY, USA, 210-219. DOI=10.1145/964001.964019 

\bibitem{joao}
Jos√© A. Joao, Onur Mutlu, and Yale N. Patt. 2009. Flexible reference-counting-based hardware acceleration for garbage collection. In Proceedings of the 36th annual international symposium on Computer architecture (ISCA '09). ACM, New York, NY, USA, 418-428. DOI=10.1145/1555754.1555806

\bibitem{hertz}
Matthew Hertz and Emery D. Berger. 2005. Quantifying the performance of garbage collection vs. explicit memory management. In Proceedings of the 20th annual ACM SIGPLAN conference on Object-oriented programming, systems, languages, and applications (OOPSLA '05). ACM, New York, NY, USA, 313-326. DOI=10.1145/1094811.1094836  

\bibitem{levanoni}
Yossi Levanoni and Erez Petrank. 2006. An on-the-fly reference-counting garbage collector for java. ACM Trans. Program. Lang. Syst. 28, 1 (January 2006), 1-69. DOI=10.1145/1111596.1111597 

\bibitem{ritzau}
Tobias Ritzau. 1999.  Real-Time Reference Counting in RT-Java

\bibitem{siek}
Jeremy G. Siek. 2011.  A Problem Course in Compilation: From Python to x86 Assembly

\end{thebibliography}

\end{document}
